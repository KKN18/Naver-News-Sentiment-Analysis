# -*- coding: utf-8 -*-
"""Crawler.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_kLO3NQFbt-xF3euxzj49ru8KMaSvGng
"""

import argparse
import torch

from bs4 import BeautifulSoup
import requests
import pandas as pd
import re

parser = argparse.ArgumentParser()
parser.add_argument('--data_dir', type=str, default='/content/drive/MyDrive/Colab Notebooks/Sentiment Analysis/data', help='directory of csv file')
parser.add_argument('--category', type=str, default='Social', help='category of search')
opt = parser.parse_args()
print(opt)

def crawler(maxpage,query,sort,s_date,e_date):
    title_text=[]
    result={}
    s_from = s_date.replace(".","")
    print(s_from)
    e_to = e_date.replace(".","")
    page = 1  
    maxpage_t =(int(maxpage)-1)*10+1  
    
    while page <= maxpage_t:
        url = "https://search.naver.com/search.naver?where=news&query=" + query + "&sort="+sort+"&ds=" + s_date + "&de=" + e_date + "&nso=so%3Ar%2Cp%3Afrom" + s_from + "to" + e_to + "%2Ca%3A&start=" + str(page)
        response = requests.get(url)
        html = response.text
 
        soup = BeautifulSoup(html, 'html.parser')
 
        contents_lists = soup.select('div.news_area > a')
        for content in contents_lists:
            title_text.append(content['title'])
        
        result= {"title":title_text}  
        
        df = pd.DataFrame(result) 
        page += 10
        print(df)
    
    
    outputFolder = opt.data_dir
    outputFileName = '%s.%s.csv' % (s_from[:4], s_from[4:6])
    df.to_csv(outputFolder + '/' + outputFileName)
    
date_list = ["29", "31", "30", "31", "30", "31", "31", "30", "31", "30", "31"]
    
def crawl_every(query):   # 2019년 12월~2021년 1월을 한 번에 크롤링하는 함수
    maxpage = 50
    sort = "0"
    s_date = "2019.12.01"
    e_date = "2019.12.31"
    crawler(maxpage,query,sort,s_date,e_date)
    for i in range(1, 10):
      s_date = "2020.0" + str(i) + ".01"
      e_date = "2020.0" + str(i) + "." + date_list[i-2]
      crawler(maxpage,query,sort,s_date,e_date) 
    for i in range(10, 13):
      s_date = "2020." + str(i) + ".01"
      e_date = "2020." + str(i) + "." + date_list[i-2]
      crawler(maxpage,query,sort,s_date,e_date) 
    s_date = "2021.01.01"
    e_date = "2021.01.31"
    crawler(maxpage,query,sort,s_date,e_date)

# query = input("검색어 입력: ")
crawl_every(opt.category)