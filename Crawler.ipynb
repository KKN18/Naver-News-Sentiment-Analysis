{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Crawler.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1_kLO3NQFbt-xF3euxzj49ru8KMaSvGng","authorship_tag":"ABX9TyOT7JUkGIL1InfcOTPFHoFP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6rMjZ0Mqia4B"},"source":["# -*- coding: utf-8 -*-\r\n","from bs4 import BeautifulSoup\r\n","from datetime import datetime\r\n","import requests\r\n","import pandas as pd\r\n","import re\r\n","\r\n","\r\n","\r\n","def crawler(maxpage,query,sort,s_date,e_date):\r\n","    title_text=[]\r\n","    result={}\r\n","    s_from = s_date.replace(\".\",\"\")\r\n","    print(s_from)\r\n","    e_to = e_date.replace(\".\",\"\")\r\n","    page = 1  \r\n","    maxpage_t =(int(maxpage)-1)*10+1  \r\n","    \r\n","    while page <= maxpage_t:\r\n","        url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\r\n","        response = requests.get(url)\r\n","        html = response.text\r\n"," \r\n","        soup = BeautifulSoup(html, 'html.parser')\r\n"," \r\n","        contents_lists = soup.select('div.news_area > a')\r\n","        for content in contents_lists:\r\n","            title_text.append(content['title'])\r\n","        \r\n","        result= {\"title\":title_text}  \r\n","        \r\n","        df = pd.DataFrame(result) \r\n","        page += 10\r\n","        print(df)\r\n","    \r\n","    \r\n","    outputFolder = '/content/drive/MyDrive/Colab Notebooks/Sentiment Analysis/data/Food/'\r\n","    outputFileName = '%s.%s.csv' % (s_from[:4], s_from[4:6])\r\n","    df.to_csv(outputFolder + outputFileName)\r\n","    \r\n","date_list = [\"29\", \"31\", \"30\", \"31\", \"30\", \"31\", \"31\", \"30\", \"31\", \"30\", \"31\"]\r\n","\r\n","def crawl_specific(): # 특정 연도와 월의 뉴스 제목을 크롤링하는 함수\r\n","    maxpage = input(\"최대 크롤링할 페이지 수 입력하시오: \")  \r\n","    query = input(\"검색어 입력: \")\r\n","    #sort = input(\"뉴스 검색 방식 입력(관련도순=0  최신순=1  오래된순=2): \")    #관련도순=0  최신순=1  오래된순=2\r\n","    sort = \"0\"\r\n","    s_date = input(\"시작날짜 입력(2019.01.04):\")  \r\n","    e_date = input(\"끝날짜 입력(2019.01.05):\") \r\n","    \r\n","    crawler(maxpage,query,sort,s_date,e_date) \r\n","    \r\n","def crawl_every():   # 2019년 12월~2021년 1월을 한 번에 크롤링하는 함수\r\n","    maxpage = 50\r\n","    query = \"음식\"\r\n","    sort = \"0\"\r\n","    s_date = \"2019.12.01\"\r\n","    e_date = \"2019.12.31\"\r\n","    crawler(maxpage,query,sort,s_date,e_date)\r\n","    for i in range(1, 10):\r\n","      s_date = \"2020.0\" + str(i) + \".01\"\r\n","      e_date = \"2020.0\" + str(i) + \".\" + date_list[i-2]\r\n","      crawler(maxpage,query,sort,s_date,e_date) \r\n","    for i in range(10, 13):\r\n","      s_date = \"2020.\" + str(i) + \".01\"\r\n","      e_date = \"2020.\" + str(i) + \".\" + date_list[i-2]\r\n","      crawler(maxpage,query,sort,s_date,e_date) \r\n","    s_date = \"2021.01.01\"\r\n","    e_date = \"2021.01.31\"\r\n","    crawler(maxpage,query,sort,s_date,e_date)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gpq1YVPquWJ5"},"source":["crawl_specific()\r\n","crawl_every()"],"execution_count":null,"outputs":[]}]}